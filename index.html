<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Resume - Start Bootstrap Theme</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
    <link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/resume.min.css" rel="stylesheet">

  </head>

  <body id="page-top">

    <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
      <a class="navbar-brand js-scroll-trigger" href="#page-top">
        <span class="d-block d-lg-none">Shanmukha Sai Shashank Garimella</span>
        <span class="d-none d-lg-block">
          <img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="img/profile.jpeg" alt="">
        </span>
      </a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#about">About</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#experience">Experience</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#education">Education</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#skills">Skills</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#awards">Certifications</a>
          </li>
        </ul>
      </div>
    </nav>

    <div class="container-fluid p-0">

      <section class="resume-section p-3 p-lg-5 d-flex d-column" id="about">
        <div class="my-auto">
          <h1 class="mb-0">Shanmukha Sai
            <span class="text-primary">Shashank Garimella</span>
          </h1>
          <div class="subheading mb-5">Dallas, Texas · (972) 360-8582 ·
            <a href="mailto:shanmukhasaishashankgarimella@gmail.com">shanmukhasaishashankgarimella@gmail.com</a>
          </div>
          <p class="mb-5">I am a Data Science professional with expertise in building scalable ETL pipelines, cloud computing, and AI/ML model deployment, leveraging platforms like Azure and AWS. Proficient in Python programming, including libraries such as Pandas, NumPy, Scikit-learn, TensorFlow, PyTorch, and LangChain, I specialize in machine learning and AI applications. I have designed and implemented data pipelines using Azure Data Factory, Apache Spark, and Delta Lake, achieving a 40% reduction in processing time while enhancing data governance. My experience includes automating repository creation and permission management workflows with Azure DevOps REST API, reducing manual setup time by 30%, and developing CI/CD pipelines for ML models using Databricks and Azure DevOps, improving delivery time by 40% and ensuring seamless integration. Additionally, I engineered AWS Lambda-based ETL pipelines for real-time processing of over 1 million records daily, achieving 99% data completeness and reliability. I have built interactive Power BI dashboards to deliver real-time insights, enhancing decision-making efficiency by 30%, and strengthened data security and compliance by implementing IAM roles, Azure Key Vault, and hierarchical namespace features for secure storage. Through performance tuning and caching strategies for Spark jobs, I optimized cloud resources and reduced infrastructure costs by 25%. I am experienced in designing AI/ML solutions for predictive modeling, image recognition, and data clustering, applying cutting-edge generative AI technologies to drive innovation and deliver impactful results.</p>
          <ul class="list-inline list-social-icons mb-0">
            <li class="list-inline-item">
              <a href="http://www.linkedin.com/in/shanmukha-sai-shashank-garimella-a04134148">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
            <li class="list-inline-item">
              <a href="https://github.com/shashank10081999">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
          </ul>
        </div>
      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="experience">
        <div class="my-auto">
          <h2 class="mb-5">Experience</h2>


          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Service Delivery Analyst</h3>
              <div class="subheading mb-3">Wheels INC</div>
              <p>•	Architected and implemented ETL pipeline using AWS Lambda, S3, and Snowflake for processing API data, serving Power BI dashboards.<br> 
                •	Developed Python scripts to extract data from API, achieving 99% data completeness and reliability.<br> 
                •	Implemented AWS CloudWatch Events for automated daily data extraction, reducing manual intervention by 95%.<br> 
                •	Engineered AWS Lambda functions for data extraction and transformation, processing 1M+ records daily.<br> 
                •	Optimized S3 bucket configurations for efficient raw and transformed data storage, reducing storage costs by 25%.<br> 
                •	Created Snow pipe automated data loading processes, ensuring near real-time data availability.<br> 
                •	Implemented data quality checks using AWS Lambda, reducing data inconsistencies by 40%.<br> 
                •	Designed transformation logic for converting raw API data into analytics-ready formats using AWS Lambda.<br> 
                •	Built Power BI dashboards for real-time analytics, improving business decision-making efficiency by 30%.<br> 
                •	Established error handling and monitoring systems using CloudWatch, achieving 99.9% pipeline reliability.<br> 
                •	Implemented IAM roles and security policies ensuring compliance with data governance standards.<br> 
                •	Created documentation for pipeline architecture and maintenance procedures, reducing onboarding time by 50%.<br> 
                •	Optimized Lambda functions for cost and performance, reducing execution time by 35%.<br> 
                •	Developed automated testing frameworks for data validation, achieving 98% accuracy in data transformation.<br> 
                •	Implemented logging and monitoring solutions across the ETL pipeline, enabling proactive issue resolution.<br>                                
                </p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">July 2023 – Present</span>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Data Engineer</h3>
              <div class="subheading mb-3">Nokia</div>
              <p>•	Designed and implemented ETL pipelines using Azure Data Factory to extract, transform, and load data from diverse data sources into Azure Data Lake Storage.<br>
                •	Automated data ingestion workflows by integrating Azure Data Factory with external APIs, reducing manual data processing efforts by 30%.<br>
                •	Developed scalable data processing solutions with Azure Databricks and Apache Spark, enabling distributed data processing and reducing job execution time by 40%.<br>
                •	Orchestrated ELT processes using Azure Data Factory and Delta Lake to transform raw data into Bronze, Silver, and Gold layers, ensuring structured data storage for downstream analytics.<br>
                •	Implemented Delta Lake architecture to handle large-scale batch and streaming data, improving data reliability and consistency by 99.9%.<br>
                •	Built reusable Python and SQL scripts for data transformation within Azure Databricks, reducing development time for new pipelines by 25%.<br>
                •	Configured Azure Data Lake Storage Gen2 to store structured and unstructured data securely with hierarchical namespace features for better performance and governance.<br>
                •	Developed data quality and validation checks within Azure Databricks, ensuring data accuracy and integrity across ingestion and transformation layers.<br>
                •	Integrated Spark MLlib libraries for data modeling and analytics, enabling predictive insights and improving operational efficiency.<br>
                •	Leveraged Azure Key Vault for securely storing and managing credentials, secrets, and keys, ensuring compliance with security policies.<br>
                •	Enabled real-time data processing using Spark Structured Streaming in Databricks, achieving a 50% improvement in processing efficiency for time-sensitive workloads.<br>
                •	Optimized Spark job performance by tuning cluster configurations and caching strategies, resulting in a 20% reduction in infrastructure costs.<br>
                •	Collaborated with cross-functional teams to define data governance frameworks, enabling effective data lineage tracking and compliance monitoring.<br>
                •	Integrated Azure Monitor and Log Analytics to monitor pipeline performance, set alerts, and provide detailed logging for debugging and performance optimization.<br>
                •	Developed CI/CD pipelines with Azure DevOps for automated deployment of data pipelines and machine learning models, ensuring seamless version control and release management.<br>
                </p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">Aug 2021 – June 2023</span>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Azure Data Engineer</h3>
              <div class="subheading mb-3">Ally Financial</div>
              <p>•Architected serverless ETL pipelines using AWS Lambda and Python for automated data extraction from REST APIs, implementing boto3 for AWS resource management.<br>
                •	Designed and implemented data lake architecture on Amazon S3 using partitioning strategies and lifecycle policies for optimized storage management.<br>
                •	Orchestrated complex data workflows using Apache Airflow and Docker containers, enabling distributed task scheduling and monitoring.<br>
                •	Developed automated data catalog solutions using AWS Glue Crawlers and Data Catalog for schema inference and metadata management.<br>
                •	Engineered PySpark applications in AWS Glue for large-scale data transformations, implementing custom UDFs and optimization techniques.<br>
                •	Implemented data analytics solutions using Amazon Athena for SQL-based analysis of S3 data lake contents.<br>
                •	Created infrastructure-as-code using AWS CloudFormation for automated deployment of data engineering resources across environments.<br>
                •	Designed fault-tolerant data pipelines with error handling and retry mechanisms using AWS Step Functions and Lambda.<br>
                •	Developed custom monitoring solutions using CloudWatch metrics and alarms for pipeline health tracking and alerting.<br>
                •	Implemented data validation frameworks using Great Expectations library integrated with AWS Glue jobs.<br>
                •	Engineered incremental data loading patterns using Spark checkpoints and AWS Glue job bookmarks for efficient processing.<br>
                •	Established data governance frameworks implementing AWS Lake Formation for fine-grained access control and resource management.<br>
                •	Built containerized data processing applications using Docker and ECR for consistent development and deployment.<br>
                •	Designed real-time data processing workflows using Kinesis and Lambda for streaming analytics.<br>
                •	Implemented automated testing frameworks for ETL processes using pytest and moto for AWS service mocking.<br>
                </p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">Jan 2020 – July 2021</span>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Azure Data Engineer</h3>
              <div class="subheading mb-3">WinCo Foods</div>
              <p>•Architected end-to-end data engineering solutions using Azure Data Factory for seamless data integration and orchestration of ETL workflows across enterprise systems.<br>
                •	Implemented Azure Databricks for large-scale data transformation, utilizing PySpark and Delta Lake to ensure reliable data processing and ACID compliance.<br>
                •	Developed robust data pipelines using Azure Data Lake Gen2 for storing raw and processed data, implementing hierarchical namespace for optimized data organization.<br>
                •	Designed and deployed Azure Synapse Analytics workspaces, integrating SQL pools and Spark pools for unified data warehousing and big data analytics.<br>
                •	Created Power BI dashboards and reports connected to Azure Synapse, utilizing DAX and Power Query for complex data modeling and visualization.<br>
                •	Established data governance frameworks using Azure Purview for data discovery, classification, and lineage tracking across the data estate.<br>
                •	Engineered automated data quality checks using Azure Functions and custom Python library Great Expectations for data validation.<br>
                •	Implemented incremental loading patterns in Azure Data Factory using Mapping Data Flows and dynamic parameters for efficient data processing.<br>
                •	Developed serverless SQL pool queries in Azure Synapse for ad-hoc analysis and reporting, optimizing query performance through materialized views.<br>
                •	Designed and implemented Azure DevOps CI/CD pipelines for automated deployment of data factory pipelines and Synapse artifacts across environments.<br>
                <br>
                </p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">July 2018 – Dec 2019</span>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Data Engineer</h3>
              <div class="subheading mb-3">Accel Frontline Limited</div>
              <p>•	Architected and implemented ETL pipelines using Apache Airflow for data processing, leveraging pandas and NumPy libraries for efficient data transformation and analysis.<br>
                •	Designed and deployed Snowflake-based data warehouse solutions utilizing Snowpipe for automated data loading from AWS S3 to target tables.<br>
                •	Developed and optimized PySpark scripts for large-scale data transformation, implementing custom UDFs with scikit-learn and TensorFlow libraries for advanced analytics.<br>
                •	Implemented containerized data processing workflows using Docker on AWS EC2, orchestrating microservices with Apache NiFi for distributed data flow management.<br>
                •	Built robust data quality frameworks using Apache Airflow DAGs and custom Python operators for automated validation checks.<br>
                •	Engineered scalable data models supporting multi-tenant architecture, implementing efficient SQL schemas that improved query performance for analytics.<br>
                •	Leveraged AWS Lambda functions with Boto3 library to automate resource management and monitoring, implementing CloudWatch metrics for system health tracking.<br>
                •	Created automated data pipeline monitoring system using AWS CloudWatch and custom Python scripts for proactive alert mechanisms.<br>
                •	Designed and implemented batch processing architecture using Apache Spark and PySpark, integrating with AWS EMR for distributed computing.<br>
                •	Developed comprehensive data governance framework using AWS IAM and S3 bucket policies, ensuring secure data access while maintaining operational efficiency.<br>                
                <br>
                </p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">May 2016 – June 2018</span>
            </div>
          </div>

        

        </div>
      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="education">
        <div class="my-auto">
          <h2 class="mb-5">Education</h2>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">University of North Texas</h3>
              <div class="subheading mb-3">Master's in Data Science</div>
              <p>GPA: 3.8</p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">Denton, TX</span>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Gandhi Institute of Technology and Management</h3>
              <div class="subheading mb-3">Bachelor of Technology in Electronics and Communication Engineering</div>
              <p>GPA: 3.61</p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">Vizag, India</span>
            </div>
          </div>

        </div>
      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="skills">
        <div class="my-auto">
          <h2 class="mb-5">Skills</h2>

          <div class="subheading mb-3">Programming Languages &amp; Tools</div>
          <ul class="list-inline list-icons">
            <li class="list-inline-item">
              <i class="devicons devicons-python"></i>
            </li>
            <li class="list-inline-item">
              <i class="devicons devicons-aws"></i>
            </li>
            <li class="list-inline-item">
              <i class="devicons devicons-docker"></i>
            </li>
            <li class="list-inline-item">
              <i class="devicons devicons-mysql"></i>
            </li>
            <li class="list-inline-item">
              <i class="devicons devicons-mongodb"></i>
            </li>
          </ul>

          <div class="subheading mb-3">Technical Skills</div>
          <ul class="fa-ul mb-0">
            <li>
              <i class="fa-li fa fa-check"></i>
              Python (Pandas, NumPy, Scikit-learn, XGBoost, Matplotlib,  Seaborn, OpenCV)</li>
            <li>
              <i class="fa-li fa fa-check"></i>
              Cloud Platforms (AWS, Azure)</li>
            <li>
              <i class="fa-li fa fa-check"></i>
              TensorFlow, PyTorch, LangChain, LlamaIndex ,Streamlit ,Flask</li>
            <li>
              <i class="fa-li fa fa-check"></i>
              Open-source and paid LLM models (Llama2, Mistral, OpenAI, Google Gemini Pro)
            <li>
              <i class="fa-li fa fa-check"></i>
              MYSQL , MongoDB , ChromaDB, Pinecone
            <li>
              <i class="fa-li fa fa-check"></i>
              Fine-tuning with custom data, vector embedding, NLP, neural network optimization, MLOPS, Docker
            <li>
              <i class="fa-li fa fa-check"></i>
              Predictive Modeling, Statistical Modelling, Clustering & Classification, Deep Learning algorithms, Computer Vision</li>
          </ul>
        </div>
      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="awards">
        <div class="my-auto">
          <h2 class="mb-5">Certifications</h2>
          <ul class="fa-ul mb-0">
            <li>
              <i class="fa-li fa fa-trophy text-warning"></i>
              AWS Certified Solutions Architect Associate SAA-C03</li>
            <li>
              <i class="fa-li fa fa-trophy text-warning"></i>
              Professional Certification on Data Science by IBM</li>
            <li>
              <i class="fa-li fa fa-trophy text-warning"></i>
              Certification on Python by University of Michigan</li>
          </ul>
        </div>
      </section>

    </div>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/resume.min.js"></script>

  </body>

</html>